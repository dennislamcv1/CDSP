{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "import sys                                                  # Read system parameters.\n",
    "import numpy as np                                          # Work with multi-dimensional arrays.\n",
    "import pandas as pd                                         # Manipulate and analyze data.\n",
    "import matplotlib                                           # Create and format charts.\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns                                       # Make charting easier.\n",
    "import sklearn                                              # Train and evaluate machine learning models.\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "                                    learning_curve, \\\n",
    "                                    cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, \\\n",
    "                            explained_variance_score, \\\n",
    "                            mean_absolute_error, \\\n",
    "                            mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import xgboost                                              # Build gradient boosting models.\n",
    "from xgboost import XGBRegressor\n",
    "import pickle                                               # Save Python objects as binary files.\n",
    "from collections import Counter\n",
    "import warnings                                             # Suppress warnings.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure results are reproducible.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- pandas {}'.format(pd.__version__))\n",
    "print('- Matplotlib {}'.format(matplotlib.__version__))\n",
    "print('- Seaborn {}'.format(sns.__version__))\n",
    "print('- scikit-learn {}'.format(sklearn.__version__))\n",
    "print('- XGBoost {}'.format(xgboost.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data.\n",
    "\n",
    "\n",
    "\n",
    "# Preview the first five rows of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable and get the count of each value in the variable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the target variable distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into target and features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into separate training and testing sets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the shape of both the training dataset and the test dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for the target variable (test data).\n",
    "# Count, mean, standard deviation, minimum, maximum, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the test data as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearRegression() model and fit it on the scaled training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get the first 5 predicted values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a quick evaluation of the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the model's R2 score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestRegressor() model and fit it on the scaled training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get the first 5 predicted values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a quick evaluation of the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the model's R2 score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare evaluation metrics for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List will hold model objects.\n",
    "\n",
    "models = []\n",
    "\n",
    "# Dummy Classifier used as a baseline algorithm.\n",
    "\n",
    "models.append(('Dummy Regressor', DummyRegressor()))\n",
    "\n",
    "# Linear Regression model.\n",
    "\n",
    "models.append(('Linear Regression', LinearRegression()))\n",
    "\n",
    "# Random Forest model.\n",
    "\n",
    "models.append(('Random Forest', RandomForestRegressor()))\n",
    "\n",
    "# XGBoost model.\n",
    "\n",
    "models.append(('XGBoost', XGBRegressor(objective = 'reg:squarederror', n_jobs = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List will hold dictionaries of model scores.\n",
    "\n",
    "scoring_df = []\n",
    "\n",
    "# Train each model in the list and output multiple scores for each model.\n",
    "\n",
    "for name, model in models:\n",
    "    if name in ['Linear Regression']:\n",
    "        X_train_1 = X_train_stand\n",
    "    else:\n",
    "        X_train_1 = X_train\n",
    "    \n",
    "    model.fit(X_train_1, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcualte the evaluation metrics for the model.\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    scoring_dict = {'Model': name,\n",
    "                    'R2': round(r2, 4),\n",
    "                    'Mean Absolute Error': round(mae, 4), \n",
    "                    'Mean Squared Error': round(mse, 4),\n",
    "                    }\n",
    "    \n",
    "    scoring_df.append(scoring_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from scoring_df.\n",
    "\n",
    "\n",
    "\n",
    "# Sort the DataFrame by MSE (ascending), then print it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin evaluating the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the lowest MSE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get the first 5 predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a residual scatter plot.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a feature importance plot for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a feature importance plot on a bar chart.\n",
    "\n",
    "def feature_importance_plot(model, X_train, n):\n",
    "    \"\"\"Plots feature importance. This only works for random forest and XGBoost models.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))  # Set figure size.\n",
    "    feat_importances = pd.Series(model.feature_importances_,\n",
    "                                 index = X_train.columns)\n",
    "    feat_importances.nlargest(n).plot(kind = 'barh')\n",
    "    plt.title(f'Top {n} Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a learning curve for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates and plots a learning curve.\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train):\n",
    "    \"\"\"Plots learning curves for model validation.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))  # Set figure size.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                            X_train,\n",
    "                                                            y_train,\n",
    "                                                            cv = 5,  # Number of folds in cross-validation.\n",
    "                                                            scoring = 'neg_mean_squared_error',  # Evaluation metric.\n",
    "                                                            n_jobs = 1,\n",
    "                                                            shuffle = True,\n",
    "                                                            train_sizes = np.linspace(0.01, 1.0, 5))  # 5 different sizes of the training set.\n",
    "\n",
    "    # Create means and standard deviations of training set scores.\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis = 1)\n",
    "    train_std = np.std(train_scores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores.\n",
    "    \n",
    "    test_mean = np.mean(test_scores, axis = 1)\n",
    "    test_std = np.std(test_scores, axis = 1)\n",
    "\n",
    "    # Draw lines.\n",
    "    \n",
    "    plt.plot(train_sizes, train_mean, '--', color = '#111111', label = 'Training score')\n",
    "    plt.plot(train_sizes, test_mean, color = '#111111', label = 'Cross-validation score')\n",
    "\n",
    "    # Create plot.\n",
    "    \n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Training Set Size'), plt.ylabel('Negative MSE'), plt.legend(loc = 'best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot learning curves for the best model.\n",
    "# Keep in mind that this is using negative MSE, so the lower (better) scores are at the top of the y-axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model as a pickle file named best_regression_model.pickle.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
