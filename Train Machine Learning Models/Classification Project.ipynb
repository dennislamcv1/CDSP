{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "import sys                                                  # Read system parameters.\n",
    "import numpy as np                                          # Work with multi-dimensional arrays.\n",
    "import pandas as pd                                         # Manipulate and analyze data.\n",
    "import matplotlib                                           # Create and format charts.\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns                                       # Make charting easier.\n",
    "import sklearn                                              # Train and evaluate machine learning models.\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "                                    learning_curve, \\\n",
    "                                    cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, \\\n",
    "                            confusion_matrix, \\\n",
    "                            classification_report, \\\n",
    "                            scorer, \\\n",
    "                            f1_score, \\\n",
    "                            recall_score, \\\n",
    "                            precision_score, \\\n",
    "                            roc_auc_score, \\\n",
    "                            plot_roc_curve, \\\n",
    "                            plot_precision_recall_curve, \\\n",
    "                            plot_confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost                                              # Build gradient boosting models.\n",
    "from xgboost import XGBClassifier\n",
    "import pickle                                               # Save Python objects as binary files.\n",
    "from collections import Counter\n",
    "import warnings                                             # Suppress warnings.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure results are reproducible.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Summarize software libraries used.\n",
    "print('Libraries used in this project:')\n",
    "print('- Python {}'.format(sys.version))\n",
    "print('- NumPy {}'.format(np.__version__))\n",
    "print('- pandas {}'.format(pd.__version__))\n",
    "print('- Matplotlib {}'.format(matplotlib.__version__))\n",
    "print('- Seaborn {}'.format(sns.__version__))\n",
    "print('- scikit-learn {}'.format(sklearn.__version__))\n",
    "print('- XGBoost {}'.format(xgboost.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data.\n",
    "\n",
    "\n",
    "\n",
    "# Preview the first five rows of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable and get the count of each value in the variable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into target and features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into separate training and testing sets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the shape of both the training dataset and the test dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Counter library to get the count of each value in the target variable (test data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression() model and fit it on the scaled training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get a count of each prediction value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a quick evaluation of the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the accuracy of the model's predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the classification_report() function to get a table of additional metric scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier() model and fit it on the scaled training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get a count of each prediction value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a quick evaluation of the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the accuracy of the model's predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the classification_report() function to get a table of additional metric scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare evaluation metrics for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List will hold model objects.\n",
    "\n",
    "models = []\n",
    "\n",
    "# DummyClassifier() used as a baseline algorithm.\n",
    "\n",
    "models.append(('Dummy Classifier', DummyClassifier(strategy = 'stratified')))\n",
    "\n",
    "# Logistic Regression model.\n",
    "\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "\n",
    "# Random Forest model.\n",
    "\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "\n",
    "# XGBoost model.\n",
    "\n",
    "models.append(('XGBoost', XGBClassifier(eval_metric = 'logloss', n_jobs = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List will hold dictionaries of model scores.\n",
    "\n",
    "scoring_df = []\n",
    "\n",
    "# Train each model in the list and output multiple scores for each model.\n",
    "\n",
    "for name, model in models:\n",
    "    if name in ['Logistic Regression']:\n",
    "        X_train_1 = X_train_norm\n",
    "    else:\n",
    "        X_train_1 = X_train\n",
    "    \n",
    "    model.fit(X_train_1, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcualte the evaluation metrics for the model.\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    scoring_dict = {'Model': name,\n",
    "                    'Accuracy': round(accuracy, 4), \n",
    "                    'F1 Score': round(f1, 4), \n",
    "                    'Precision' : round(precision, 4), \n",
    "                    'Recall' : round(recall, 4), \n",
    "                    'AUC' : round(auc ,4), \n",
    "                   }\n",
    "    \n",
    "    scoring_df.append(scoring_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from scoring_df.\n",
    "\n",
    "\n",
    "\n",
    "# Sort the DataFrame by accuracy score (descending), then print it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin evaluating the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the highest accuracy score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "\n",
    "\n",
    "\n",
    "# Get a count of each prediction value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a ROC curve.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a confusion matrix of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a feature importance plot for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates a feature importance plot on a bar chart.\n",
    "\n",
    "def feature_importance_plot(model, X_train, n):\n",
    "    \"\"\"Plots feature importance. This only works for random forest and XGBoost models.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))  # Set figure size.\n",
    "    feat_importances = pd.Series(model.feature_importances_,\n",
    "                                 index = X_train.columns)\n",
    "    feat_importances.nlargest(n).plot(kind = 'barh')\n",
    "    plt.title(f'Top {n} Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a learning curve for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates and plots a learning curve.\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train):\n",
    "    \"\"\"Plots learning curves for model validation.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))  # Set figure size.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                            X_train,\n",
    "                                                            y_train,\n",
    "                                                            cv = 5,  # Number of folds in cross-validation.\n",
    "                                                            scoring = 'accuracy',  # Evaluation metric.\n",
    "                                                            n_jobs = 1,\n",
    "                                                            shuffle = True,\n",
    "                                                            train_sizes = np.linspace(0.01, 1.0, 5))  # 5 different sizes of the training set.\n",
    "\n",
    "    # Create means and standard deviations of training set scores.\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis = 1)\n",
    "    train_std = np.std(train_scores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores.\n",
    "    \n",
    "    test_mean = np.mean(test_scores, axis = 1)\n",
    "    test_std = np.std(test_scores, axis = 1)\n",
    "\n",
    "    # Draw lines.\n",
    "    \n",
    "    plt.plot(train_sizes, train_mean, '--', color = '#111111', label = 'Training score')\n",
    "    plt.plot(train_sizes, test_mean, color = '#111111', label = 'Cross-validation score')\n",
    "    \n",
    "    # Create plot.\n",
    "    \n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Training Set Size'), plt.ylabel('Accuracy'), plt.legend(loc = 'best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot learning curves for the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model as a pickle file named best_classification_model.pickle.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
